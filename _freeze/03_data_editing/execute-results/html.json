{
  "hash": "0a4edd66ecdb662c9d1565933752ceb0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Processamento de Dados\"\nauthor:\n  - name: Paulo Barros\n    orcid: 0000-0001-9839-0796\n    email: pbarrosbio@gmail.com\n    affiliations:\n      - name: PPZ - UESB - GACOM\n\n---\n\n\n\n\n\n\nO processamento de dados é sem dúvida a etapa mais importante e que mais consome\ntempo em um fluxo de análises. Nesta seção vamos abordar algumas funções básicas\ndo `tidyverse` que auxiliam bastante na realização de tarefas triviais de edição\nde dados.\n\n## Combinando _Datasets_\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n:::\n\n\n\n\n\n\n\nEm muitas situações é comum que seja necessário combinar múltiplas fontes de dados.\nEm melhoramento animal por exemplo é comum que informações dos mesmos animais estejam\ndispersas por arquivos de dados individuais.\n\nVamos dar uma olhada nos datasets que temos disponíveis? Se lembram como fazer?\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlist.files(path = \"data\", pattern = \"NEL.*.csv\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"NELP365.csv\" \"NELP550.csv\" \"NELPN.csv\"  \n```\n\n\n:::\n:::\n\n\n\n\n\n\nOs arquivos `NELPN.csv`, `NELP365.csv` e `NELP550.csv` possuem dados de 50 animais com\nrespectivas medidas biométricas para Peso ao Nascimento (PN), Peso a um ano\n(P365) e Peso ao Sobreano (P550). Estes dados foram simulados com base em animais\nda raça Nelore.\n\nUma vez que os mesmos animais estão nos três conjuntos de dados, nosso objetivo\né unir estes arquivos em um único conjunto de dados. Vamos fazer isso de duas\nmaneiras diferentes.\n\n### Modo Básico\n\n* Carregamos os arquivos individualmente cada um em um objeto\n\n* Usamos as funções de _join_ do `tidyverse` para unir os objetos\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npn <- read_csv(\"data/NELPN.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 50 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): ID\ndbl (1): PN\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nglimpse(pn)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 50\nColumns: 2\n$ ID <chr> \"RV143\", \"YB821\", \"QA008\", \"WT997\", \"IT185\", \"PD544\", \"SL754\", \"MC3…\n$ PN <dbl> 36.04, 31.09, 32.33, 28.35, 25.06, 37.53, 33.23, 24.19, 30.05, 37.9…\n```\n\n\n:::\n\n```{.r .cell-code}\np365 <- read_csv(\"data/NELP365.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 50 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): ID\ndbl (1): P365\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nglimpse(p365)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 50\nColumns: 2\n$ ID   <chr> \"RV143\", \"YB821\", \"QA008\", \"WT997\", \"IT185\", \"PD544\", \"SL754\", \"M…\n$ P365 <dbl> 211.15, 194.81, 186.59, 192.42, 202.37, 189.50, 197.18, 160.95, 2…\n```\n\n\n:::\n\n```{.r .cell-code}\np550 <- read_csv(\"data/NELP550.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 50 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): ID\ndbl (1): P550\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nglimpse(p550)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 50\nColumns: 2\n$ ID   <chr> \"RV143\", \"YB821\", \"QA008\", \"WT997\", \"IT185\", \"PD544\", \"SL754\", \"M…\n$ P550 <dbl> 290.43, 304.21, 305.59, 285.34, 331.91, 273.09, 295.91, 335.95, 2…\n```\n\n\n:::\n:::\n\n\n\n\n\n\nA função `glimpse` é bastante útil pra nos mostrar uma prévia dos nossos dados.\n\nAgora que nossos arquivos já foram carregados, podemos fazer a combinação (_merge_)\ndos dados. Para este tipo de operação no qual temos os mesmos animais e variáveis\nmedidas em arquivos separados, usaremos a ID de cada animal como uma chave de\nidentificação entre os conjuntos de dados, desta maneira sabemos que teremos\ncada observação de cada animal corretamente alocada no nosso novo conjunto de dados.\n\nNo nosso caso usaremos a função `inner_join` do `dplyr`. Esta função recebe dois\nconjuntos de dados e combina as observações somente para os índices em comum em\nambos os arquivos. Como temos três datasets, usaremos o pipe para fazer a operação\nem um único fluxo de código.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndados_nelore <- inner_join(pn,p365, by = join_by(ID)) |>\n  inner_join(p550, by = join_by(ID))\n\ndados_nelore |>\n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 4\n  ID       PN  P365  P550\n  <chr> <dbl> <dbl> <dbl>\n1 RV143  36.0  211.  290.\n2 YB821  31.1  195.  304.\n3 QA008  32.3  187.  306.\n4 WT997  28.4  192.  285.\n5 IT185  25.1  202.  332.\n6 PD544  37.5  190.  273.\n```\n\n\n:::\n:::\n\n\n\n\n\n\nParece confuso, mas é bem simples. Vamos por partes.\n\n`inner_join(pn,p365, by = join_by(ID))` : primeiro passamos os nossos conjuntos\nde dados como argumentos para a função `inner_join`, e com o argumento `by`\nfazemos a definição de qual variável será nossa chave de identificação ou índice,\nneste caso com `by = join_by(ID)` estamos informando que a coluna `ID` é a chave.\n\nAo fazermos isso já criamos um novo objeto que é a junção dos dados de PN e P365,\ne como mencionamos anteriormente, o `|>` lê sempre da esquerda para a direita,\nassim o nosso novo conjunto de dados do merge se torna a entrada da próxima função\n\n`inner_join(p550, by = join_by(ID))` : aqui o raciocínio é o mesmo, com a diferença\nde que como estamos no fluxo do pipe, o primeiro argumento é omitido pois o R já\nsabe que esse argumento vem do pipe anterior, e informamos então o conjunto de\ndados restante `p550`, a chave continua a mesma. E assim nosso merge está completo!\n\nExistem outros tipos de _joins_ no `dplyr` e você pode como sempre \n[consultar a documentação](https://dplyr.tidyverse.org/reference/mutate-joins.html){target=\"_blank\"}\npara saber os detalhes e diferenças entre eles e quando utilizar cada tipo.\n\n![Tipos de _Join_. Fonte: [pozdniakov.github.io](https://pozdniakov.github.io/tidy_stats_eng/images/joins.png){target=\"_blank\"}](https://pozdniakov.github.io/tidy_stats_eng/images/joins.png){width=\"400\"}\n\n### Modo Otimizado\n\nNo nosso exemplo temos somente três datasets que desejamos unir, mas existem\nsituações aonde o número de datasets pode ser grande bem como o volume de dados\ncontidos neles. Nessas situações podemos lançar mão de funções otimizadas para\nrealizar tarefas repetitivas. A função `map` do pacote `purrr` é nossa amiga!\n\nVamos realizar a mesma operação do modo anterior mas de uma maneira mais eficiente\ntanto do ponto de vista de código quanto de gerenciamento de recursos do computador.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnelore <- list.files(path = \"data\",\n                     pattern = \"NEL.*.csv\",\n                     full.names = TRUE) |>\n  map(read_csv) |>\n  reduce(inner_join, by = join_by(ID)) |>\n  select(ID,PN,P365,P550)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 50 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): ID\ndbl (1): P365\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 50 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): ID\ndbl (1): P550\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 50 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): ID\ndbl (1): PN\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nnelore |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 4\n  ID       PN  P365  P550\n  <chr> <dbl> <dbl> <dbl>\n1 RV143  36.0  211.  290.\n2 YB821  31.1  195.  304.\n3 QA008  32.3  187.  306.\n4 WT997  28.4  192.  285.\n5 IT185  25.1  202.  332.\n6 PD544  37.5  190.  273.\n```\n\n\n:::\n:::\n\n\n\n\n\n\nVamos por parte novamente!\n\nO primeiro passo foi gerar uma lista de arquivos a serem lidos automaticamente\nusando a função `list.files`. Com o argumento `pattern = \"NEL.*.csv\"` nós informamos\nque queremos listar somente os arquivos que comecem com NEL e sejam `.csv`. E por\nfim usamos `full.names = TRUE` para que ele nos retorne o caminho completo do\narquivo incluindo o diretóriom p.e. `data/NELPN.csv`.\n\nCom isso passamos o nosso vetor contendo os nomes dos arquivos para a função `map`.\nEsta função recebe uma lista/vetor e caminha pelos itens desta lista realizando a\noperação solicitada para cada item, no nosso caso irá executar a função `read_csv`\npara cada nome de arquivo informado no nosso vetor.\n\nA função `map` retorna uma lista por padrão, no nosso caso uma lista de `data.frame`.\nPor isso ao final invocamos a função `reduce` para reduzir a nossa lista aplicando\na função `inner_join` da mesma maneira que fizemos no modo anterior.\n\nNeste exemplo pode parecer que o Modo 1 tem menos linhas de código e seja mais\nfácil, e de fato é. Entretanto, imagine se precisássemos ler 50 arquivos ao invés\nde três? Precisariamos criar 50 objetos e fazer o join individualmente destes, o\nque aumentaria substancialmente o número de linhas e também a quantidade de\nmemória utilizada na operação.\n\n\n## Transformando Dados\n\nUma outra operação muito comum em edição de dados é transformar o formato do\nconjunto de dados entre **formato longo (_long_)** e **formato largo (_wide_)**.\nPara demonstrar isso vamos recuperar nosso dataset de galinhas.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readxl)\nlibrary(janitor)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'janitor'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n```\n\n\n:::\n\n```{.r .cell-code}\ngalinhas <- read_xlsx(\"data/dietas_galinha.xlsx\") |>\n  clean_names()\n\nhead(galinhas)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 6\n    rep dieta_a dieta_b dieta_c dieta_d dieta_e\n  <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1     1     179     309     243     423     368\n2     2     160     229     230     340     390\n3     3     136     181     248     392     379\n4     4     227     141     327     339     260\n5     5     217     260     329     341     404\n6     6     168     203     250     226     318\n```\n\n\n:::\n:::\n\n\n\n\n\n\nEste dataset contem peso de galinhas submetidas a diferentes dietas. Como podemos\nobservar ele se encontra no formato que chamamos de **largo (_wide_)**, uma vez\nque cada dieta que seria o nosso tratamento está representada em uma coluna\nseparada.\n\nNa filosofia do `tidyverse`, para um dataset ser considerado **_tidy_** ou \"arrumado\",\ncada célula deve ser uma **observação completa**. Para isso podemos transformar\no nosso formato para formato **longo (_long_)**.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngal_long <- galinhas |>\n  pivot_longer(dieta_a:dieta_e, names_to = \"dieta\", values_to = \"peso\") |>\n  arrange(dieta)\n\ngal_long |>\n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 3\n    rep dieta    peso\n  <dbl> <chr>   <dbl>\n1     1 dieta_a   179\n2     2 dieta_a   160\n3     3 dieta_a   136\n4     4 dieta_a   227\n5     5 dieta_a   217\n6     6 dieta_a   168\n```\n\n\n:::\n:::\n\n\n\n\n\n\nA função `pivot_longer` recebe um intervalo de colunas `dieta_a:dieta_e` no nosso\ncaso, e transforma em uma nova variável que chamamos de `dieta` e os valores em\numa outra chamada `peso`. Por fim usamos a função `arrange` para ordenar nossos\ndados pela coluna da dieta.\n\nTambém podemos fazer o caminho inverso agora. \n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngal_long |>\n  pivot_wider(names_from = dieta, values_from = peso) |>\n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 6\n    rep dieta_a dieta_b dieta_c dieta_d dieta_e\n  <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1     1     179     309     243     423     368\n2     2     160     229     230     340     390\n3     3     136     181     248     392     379\n4     4     227     141     327     339     260\n5     5     217     260     329     341     404\n6     6     168     203     250     226     318\n```\n\n\n:::\n:::\n\n\n\n\n\n\nAqui a função  `pivot_wider` recebe uma coluna com valores categóricos e cria\ncolunas individuais para cada valor na variável, associando o valor de `peso`\ncorrespondente. Desta forma recuperamos o formato _wide_ que existia nos dados\noriginais.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}